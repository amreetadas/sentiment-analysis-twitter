---
title: "Sentiment Analysis of Tweets leading up to the U.S. 2020 Presidential Elections"
author: "Amreeta P. Das"
output:
  html_document:
    df_print: paged
  pdf_document: default
  word_document: default
---

# Introduction #

This project delves into the sentiment analysis of the expressed public opinion on Twitter in the run-up to the U.S. 2020 Presidential elections. Given the readily available data on Twitter, it serves as the primary platform for this analysis, but other social media outlets could provide further insights in subsequent studies.

# Data Sources #
1. Labelled Twitter Sentiment Data: Sentiment 140 https://www.kaggle.com/datasets/kazanova/sentiment140/data
2. 2020 US Presidential Elections Dataset: https://www.kaggle.com/datasets/manchunhui/us-election-2020-tweets


# Step 1: We will work on the Sentiment140 dataset to build our ML Model. #

# Load libraries # 

``` {r}
library(tensorflow)
library(keras)
library(here)
library(skimr)
library(ggplot2)
library(tm)
library(dplyr)
library(jsonlite)
library(textcat)
library(visdat)
library(lubridate)
library(wordcloud)
library(maps)
library(sf)
```

# Load Data #

```{r}
path <- here::here() 


#load labelled sentiment data
senti <- here(path, 
              "Data", 
              "training.1600000.processed.noemoticon.csv") |> 
    read.csv(fileEncoding = 'MACROMAN')

#rename columns
colnames(senti) <- c("target", "ids", "date", "flag", "user", "text")
```

# Overview of data #

```{r}
head(senti)
```

```{r}
skim(senti)
```

```{r}
dim(senti)
```

*No missing values in any of our columns, which is great. N = ~1.6 million (Also good, but might need to work with a sample to make it work on system.)*

# Overview of 'target' which is the outcome of interest #

``` {r}
unique(senti$target) 
```
*Unique values in 'target' is 0 and 4, 0 = negative, 4 = positive. We will need to re-code 4 to 1 so that it is a binary variable.*

```{r}
senti$target <- ifelse(senti$target == 4, 1, 0)
```

*Count and plot the distribution of positive and negative tweets in the dataset.*

```{r}
table(senti$target) #equal number of positive and negative tweets
```

*Use sample of data to make it easier for system to handle*

```{r}
set.seed(1234)
senti_sample <- sample_n(senti, 500000)
```

*Plot distribution of target variable in sample*
```{r}

options(scipen = 999) #turns off scientific notation in y-axis
ggplot(senti_sample, aes(target)) +
  geom_bar() +
  scale_x_discrete(limits=c(0,1), labels=c("Negative","Positive")) +
  labs(title="Distribution of sentiment in dataset", y = "Count", x = "Sentiment") 

```

# Data preprocessing #

*Clean text*

``` {r}
#create function to clean text 

clean_text <- function(text) {
  text <- gsub("RT", "", text) # Remove RT
  text <- gsub("@\\w+", "", text) # Remove usernames
  text <- gsub("http\\S+\\s*", "", text) # Remove links
  text <- gsub("[[:punct:]]", "", text) # Remove punctuation
  text <- gsub("[[:digit:]]", "", text) # Remove digits
  text <- gsub("^[[:space:]]*","",text) # Remove leading whitespaces
  text <- gsub("[[:space:]]*$","",text) # Remove trailing whitespaces
  text <- tolower(text) # Convert to lowercase
  return(text)
}

#apply to sample
senti_sample$text <- sapply(senti_sample$text, clean_text)

```

*Tokenization and padding sequences*

```{r}

mean(nchar(senti_sample$text)) #62 is the avg number of characters in the tweets in our dataset

size <- 65 #max sequence length based on the mean
tokenizer <- text_tokenizer() %>% fit_text_tokenizer(senti_sample$text) #fitted tokenizer to dataset
data_x <- texts_to_sequences(tokenizer, senti_sample$text) %>% pad_sequences(maxlen = size) #convert raw text to sequences of integers
data_y <- senti_sample$target #defining sentiment column

```

*Split dataset into train and test set*

```{r}

train_size <- floor(0.8*nrow(senti_sample)) ## allocate 80% of data to training set
train_id <- sample(c(1:nrow(data_x)), train_size) # sample rows = train_size
test_id <- (1:nrow(data_x))[-train_id] #those rows not included in train_id

```

# Build and train model #

I will be using a bi-directional Long Short-Term Memory (LSTM) model for the classification. LSTM is a type of Recurrent Neural Network (RNN) that are well-suited for processing sequences, including textual data like tweets. 

```{r}
#Build model 
model <- keras_model_sequential() %>%
  layer_embedding(input_dim = 5000, output_dim = 128) %>%
  bidirectional(layer_lstm(units = 64)) %>% 
  layer_dropout(rate = 0.2) %>% 
  layer_dense(units = 1, activation = "sigmoid")

#Compile the model
model %>% compile(
  loss = 'binary_crossentropy',
  optimizer = 'adam',
  metrics = c('accuracy')
)

#Train the model - this might take a long time to run. If it does, reduce sample size and rerun the previous steps.

history <- model %>% fit(
  data_x[train_id,], data_y[train_id],
  epochs = 6,
  batch_size = 128,
  validation_split = 0.2
)


plot(history)


```



# Test model #

``` {r}

test_prediction <- model %>% predict(data_x[test_id,]) #use the model to predict on test set
test_true <- data_y[test_id] #real target values in test set
raw_value <- senti_sample[test_id,6] #tweet

temp <- data.frame(prediction=test_prediction, true=test_true, raw=raw_value) #put the relevant columns together in the dataset
temp$prediction <- ifelse(temp$prediction >.5,1, 0) #change from predicted probability to 0-1. if higher than 0.5 it is a 1.

mean(temp$prediction==temp$true) ## accuracy rate 
mean(temp$true[temp$prediction > .5]==1) ## share of positives that are actually positive = 82%
mean(temp$true[temp$prediction > .5]==0) ## share of positives that are not actually positive = 18%

```


# Save model #

```{r}
model_name <- paste0(path,"/Models/LSTM",Sys.Date(),".h5")
save_model_hdf5(model, model_name)

tokenizer_config <- tokenizer$word_index
tokenizer_file <- paste0(path,"/Models/tokenizer",Sys.Date(),".json")
write_json(tokenizer_config, tokenizer_file)

```


# Step 2: Apply model to 2020 elections tweets datasets #

*Load datasets*

```{r}
trump <- read.csv(paste0(path, "/Data/elections/hashtag_donaldtrump.csv"))

biden <- read.csv(paste0(path, "/Data/elections/hashtag_joebiden.csv"))

```

Before applying the sentiment prediction model to the datasets, I will clean and trim the data down a bit to make it less computationally intensive. 

*Preprocess the datasets*

``` {r}
trump <- trump %>% 
  filter(country %in% c("United States of America", "United States")) %>% #filter down to tweets from USA
  mutate(tweet = sapply(tweet, clean_text)) %>% #clean_text using same function as before
  mutate(language = textcat(tweet)) %>% #detect language of tweets using textcat function
  filter(language == "english") #filter and keep only english language tweets

biden <- biden %>% 
  filter(country %in% c("United States of America", "United States")) %>% #filter down to tweets from USA
  mutate(tweet = sapply(tweet, clean_text)) %>% #clean_text using same function as before
  mutate(language = textcat(tweet)) %>% #detect language of tweets using textcat function
  filter(language == "english") #filter and keep only english language tweets
```

*Tokenize and pad the datasets*

```{r}
#load the model using load_model_hdf5(filepath) if needed to load again

model <- load_model_hdf5(paste0(path, "/Models/LSTM2023-11-14.h5"))

### Trump data ###

size <- 65 #max sequence length
tokenizer_trump <- text_tokenizer() %>% fit_text_tokenizer(trump$tweet) #fitted tokenizer to dataset
data_x_trump <- texts_to_sequences(tokenizer_trump, trump$tweet) %>% pad_sequences(maxlen = size) #convert raw text to sequences of integers
trump_id <- (1:nrow(data_x_trump)) #row identifiers
trump$prediction <- model %>% predict(data_x_trump[trump_id,]) #apply model to new data
trump$sentiment <- ifelse(trump$prediction > 0.5, 1, 0) #1 or 0 

### Biden data ###

size <- 65 #max sequence length
tokenizer_biden <- text_tokenizer() %>% fit_text_tokenizer(biden$tweet) #fitted tokenizer to dataset
data_x_biden <- texts_to_sequences(tokenizer_biden, biden$tweet) %>% pad_sequences(maxlen = size) #convert raw text to sequences of integers
biden_id <- (1:nrow(data_x_biden)) #row identifiers
biden$prediction <- model %>% predict(data_x_biden[biden_id,]) #apply model to new data
biden$sentiment <- ifelse(biden$prediction > 0.5, 1, 0) #1 or 0 

```


# Step 3: Exploratory Data Analysis on 2020 Presidential Election Tweets #

First, I will add a column to each df to identify who the tweet is about and bind the two dfs together.

```{r}

trump <- trump %>%
  mutate(about = "trump")

biden <- biden %>%
  mutate(about = "biden")

election_df <- rbind(trump, biden)

```

Visualize distribution of who the tweets are about.

```{r}
options(scipen = 999) #turns off scientific notation in y-axis
ggplot(election_df, aes(about)) +
  geom_bar() +
  labs(title="Distribution of tweets in dataset", y = "Count", x = "About") 
```
**Notable trends**
We observe that in this sample, the number of tweets mentioning Trump is more than the number of tweets mentioning Biden.


In this dataset, instead of NAs we have empty strings. So first we will convert the empty strings to NAs.

```{r}
#Convert empty strings to NA
election_df <- election_df %>% mutate_all(~replace(., . == "" | . == " ", NA))

```

Visualize missingness with visdat (sample)
```{r}

#take a slice of the data and visualize missingness
election_df_sample <- slice_sample(election_df, n = 10000)
visdat::vis_miss(election_df_sample)


```

Visualize missingness in whole df.
```{r}

# Convert to data frame
missing_df <- data.frame(column = names(election_df), missing_count = sapply(election_df, function(x) sum(is.na(x))))

# Plot
ggplot(missing_df, aes(x = column, y = missing_count)) +
  geom_bar(stat = "identity", fill = "steelblue") +
  labs(title = "Missing Values by Column", y = "Number of Missing Values", x = "Columns") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

```

**Notable trends**
From the two visualizations above, we see there is most amount of missingness in the geographic variables: city, state, state_code. There is also missingness in user description. We won't be using city and user description, so that is not a problem.


# Number of tweets every day #

```{r}

class(election_df$created_at) #character

election_df$created_at <- as_datetime(election_df$created_at, tz = "UTC") #convert into date-time


# Extract day from date-time and count number of tweets per day
tweets_daily <- election_df %>%
  mutate(date = lubridate::date(created_at)) %>%
  group_by(date, about) %>%
  summarise(count = n()) %>%
  ungroup

# Plot
ggplot(tweets_daily, aes(x = date, y = count, color = about)) +
  geom_line() +
  labs(title = "Count of Tweets Daily", y = "Count of Tweets", x = "Date") +
  scale_x_date(date_breaks = "1 day", date_labels = "%m/%d/%y") +
  theme(axis.text.x=element_text(angle=90))


```
**Notable trends**
We notice a couple of things. First, the number of tweets per day mentioning both Biden and Trump increase over time as the election on November 3 draws near. Next, the number of tweets peaks between October 15-16 and October 22-23. The former was when a debate between the Presidential candidates was supposed to happen but was ultimnately cancelled, and the latter is when the last presidential debate took place. There is also a peak in tweets about Biden on November 7, which is when it was officially announced that Biden was elected as President.


# Word Clouds #

Here, we can visualize the most commonly used worked in the positive and negative tweets to get a sense about what kind of issues are being talked about.

Note:this might take a couple of minutes to run.

```{r}
# Separate data based on sentiment score
positive_tweets <- election_df %>% 
  filter(sentiment == 1) %>% 
  select(tweet)

negative_tweets <- election_df %>% 
  filter(sentiment == 0) %>% 
  select(tweet)

# Positive Tweets
wordcloud(positive_tweets, max.words = 100, scale=c(3,0.5), colors=brewer.pal(8, "Dark2"))
title(main="Positive Tweets", line=-1, cex.main=2)

# Negative Tweets
wordcloud(negative_tweets, max.words = 100, scale=c(3,0.5), colors=brewer.pal(8, "Dark2"))
title(main="Negative Tweets", line=-1, cex.main=2)
```

**Notable trends**
From the word clouds alone it is hard to ascertain what topics are being discussed. 
1. Positive tweets word cloud: The most common words in the word cloud are "America(ns)," "maga", "Trump" and "Biden". This could be suggestive of positive tweets about Trump who is "making America great again". There are policy specific terms like "covid" included too.
2. Negative tweets word cloud: There are policy specific terms like "covid" which in this context could indicate negative tweets about covid-19 policy in America.

# Distribution of positive and negative tweets #

```{r}

# Extract day from date-time and count number of tweets per day
tweets_daily <- election_df %>%
  mutate(date = lubridate::date(created_at)) %>%
  group_by(date, about) %>%
  summarise(count = n()) %>%
  ungroup

ggplot(election_df, aes(x = about, fill = factor(sentiment))) +
  geom_bar() +
  labs(title = "Distribution of sentiment", y = "Number of Tweets", x = "About")


```
**Notable trends**
Oevrall, it appears that tweets that mentioned Trump had a higher proportion of negative sentiment than positive. Visually, negative and positive sentiments look about equally distributed for Biden.


# Average sentiment score daily #

```{r}

# Calculate daily proportion of positive sentiments
daily_sentiment_avg <- election_df %>%
  mutate(date = lubridate::date(created_at)) %>%
  group_by(date, about) %>%
  summarise(average_sentiment = mean(sentiment, na.rm = TRUE)) %>%
  ungroup()

# Visualization
ggplot(daily_sentiment_avg, aes(x = date, y = average_sentiment, color = about)) +
  geom_line() +
  geom_point() +
  labs(title = "Average sentiment scores",
       y = "Proportion Positive", x = "Date") +
  scale_y_continuous(labels = scales::percent) +
  theme_minimal()
```
**Notable trends**
Some trends emerge here. First, on average, the sentiment for Biden appears to be higher than that of Trump in our dataset. Second, despite there be being many peaks and valleys, the sentiment for Biden has generally gone up over-time. This is especially true after 3 Nov when the election took place, peaking around 7 Nov when the election results were announced. It could be that Biden's supporters who were not vocal on Twitter, became vocal after the announcements of results. This is also mirrored in the dip in Trump's average sentiment score around the same time. 


# Average sentiment score by state #
Here, we will ignore the missing values for now.

```{r}

# Calculate daily proportion of positive sentiments
state_sentiment_avg <- election_df %>%
  group_by(state, about) %>%
  summarise(average_sentiment = mean(sentiment, na.rm = TRUE)) %>%
  ungroup()

# Visualization
ggplot(state_sentiment_avg, aes(x = state, y = average_sentiment, group = about, color = about)) +
  geom_point() +
  geom_path() +
  labs(title = "Average sentiment scores by state",
       y = "Average Sentiment", x = "State") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1))
```
**Notable trends**
This is the most interesting for out analyses, revealing clear spatial patterns.
For example, Trump has negative sentiment scores in California, Colorado, New York, vermont and Connecticut. These are all states that have a large urban population and that have traditionally voted Democratic. On the other hand, trump has greater sentiment scores in Wyoming, Texas, North Dakota, to name a few. These are all states that have a large rural population and that have traditionally voted Republican.This is consistent with other polls and data that has shown that Trump is more popular in more rural and conservative areas of the country.



# Next Steps #
As next steps, we can look at the interaction of sentiments and engagement on twitter. Does positive/negative messaging lead to more/less engagement? Another area I will be interested to analyze in the future is some kind of geospatial analysis to track the spatial distribution of average sentiment scores for each candidate. Connecting this with electoral data can yield interesting results to see if/how expressed online sentiments translate to electoral activity.






